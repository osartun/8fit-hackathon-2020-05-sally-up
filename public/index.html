<!DOCTYPE html>
<html>
  <head>
    <title>8fit Sally Up!</title>
    <link rel="stylesheet" href="index.css" />
    <link
      href="https://fonts.googleapis.com/css2?family=Public+Sans:wght@300;400;500&display=swap"
      rel="stylesheet"
    />
  </head>
  <body>
    <div>
      <h1>Sally Up! Challenge</h1>
    </div>
    <div>
      <audio src="moby-flower-sally-up.mp3" id="sally-up-mp3"></audio>
    </div>
    <div>
      <canvas
        class="video-container"
        id="canvas"
        width="400"
        height="400"
      ></canvas>
    </div>
    <div
      style="
        display: flex;
        flex-direction: row;
        justify-content: space-between;
        width: 400px;
      "
    >
      <span class="current-position" id="current-position" />
      <span id="score" class="score"></span>
    </div>
    <button type="button" class="start-button" onclick="start()" disabled>
      GO!
    </button>
    <button type="button" class="pause-button" onclick="pause()">Pause</button>
    <script src="sally-up-data.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.3.1/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@teachablemachine/pose@0.8.3/dist/teachablemachine-pose.min.js"></script>
    <script type="text/javascript">
      const currentPositionEl = document.getElementById("current-position");
      const scoreEl = document.getElementById("score");
      // More API functions here:
      // https://github.com/googlecreativelab/teachablemachine-community/tree/master/libraries/pose

      // the link to your model provided by Teachable Machine export panel
      const URL = "https://teachablemachine.withgoogle.com/models/6iVP_iE9X/";
      let model,
        webcam,
        ctx,
        labelContainer,
        maxPredictions,
        gameStatus,
        audio,
        hasStarted,
        isInitialized,
        isPaused;

      const DOWN = "down";
      const UP = "up";
      const sallyUpData = window.sallyUpData;

      async function setup() {
        const modelURL = URL + "model.json";
        const metadataURL = URL + "metadata.json";

        // load the model and metadata
        // Refer to tmPose.loadFromFiles() in the API to support files from a file picker
        model = await tmPose.load(modelURL, metadataURL);
        maxPredictions = model.getTotalClasses();

        // Convenience function to setup a webcam
        const flip = true; // whether to flip the webcam
        webcam = new tmPose.Webcam(400, 400, flip); // width, height, flip

        // append/get elements to the DOM
        const canvas = document.getElementById("canvas");
        ctx = canvas.getContext("2d");

        // Initialize audio & gameStatus
        audio = document.getElementById("sally-up-mp3");
        gameStatus = document.getElementById("game-status");

        document.querySelector(".start-button").disabled = false;
      }

      async function init() {
        await webcam.setup(); // request access to the webcam
        webcam.play();
        window.requestAnimationFrame(loop);
      }

      async function loop(timestamp) {
        if (isPaused || !isInitialized) return;
        webcam.update(); // update the webcam frame
        await predict();
        window.requestAnimationFrame(loop);
      }

      async function start() {
        if (!isInitialized) {
          isInitialized = true;
          await init();
          isPaused = false;
          return;
        }
        if (isPaused) {
          isPaused = false;
          window.requestAnimationFrame(loop);
          audio.play();
        }
      }

      function pause() {
        isPaused = true;
        webcam.pause();
        audio.pause();
      }

      async function predict() {
        // Prediction #1: run input through posenet
        // estimatePose can take in an image, video or canvas html element
        const { pose, posenetOutput } = await model.estimatePose(webcam.canvas);
        // Prediction 2: run input through teachable machine classification model
        const prediction = await model.predict(posenetOutput);
        const probabilityDown = prediction[0].probability;
        const probabilityUp = prediction[1].probability;

        let currentPosition;
        if (probabilityDown > 0.95) {
          currentPosition = DOWN;
        } else if (probabilityUp > 0.95) {
          currentPosition = UP;
        }
        currentPositionEl.className = `current-position ${currentPosition}`;

        const currentSecond = parseInt(audio.currentTime);
        const currentPositionInData = currentSecond * 5;
        const sallyUpDataCurrent = [
          sallyUpData[currentPositionInData - 2],
          sallyUpData[currentPositionInData - 1],
          sallyUpData[currentPositionInData],
          sallyUpData[currentPositionInData + 1],
          sallyUpData[currentPositionInData + 2],
          sallyUpData[currentPositionInData + 3],
        ]
          .filter((entry) => entry !== undefined)
          .map((isDown) => (isDown ? DOWN : UP));
        let score;
        if (sallyUpDataCurrent.includes(currentPosition)) {
          score = "hit";
        } else if (currentPosition !== undefined) {
          score = "miss";
        }
        scoreEl.className = `score ${score}`;
        // finally draw the poses
        drawPose(pose);

        // Start audio if this is the first time this runs
        if (!hasStarted) {
          audio.play();
          hasStarted = true;
        }
      }

      function drawPose(pose) {
        ctx.drawImage(webcam.canvas, 0, 0);
        // draw the keypoints and skeleton
        if (pose) {
          const minPartConfidence = 0.5;
          tmPose.drawKeypoints(pose.keypoints, minPartConfidence, ctx);
          tmPose.drawSkeleton(pose.keypoints, minPartConfidence, ctx);
        }
      }

      window.requestAnimationFrame(setup);
    </script>
  </body>
</html>
